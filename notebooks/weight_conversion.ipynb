{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3104b665-faa5-44fa-b689-8584ad0f1912",
   "metadata": {},
   "source": [
    "# Borzoi weight conversion from TF to Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "084764e4-3fef-4d05-8d99-061b51049c30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.config.experimental.enable_tensor_float_32_execution(False)\n",
    "import baskerville\n",
    "from baskerville import seqnn\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd0c265d-adae-4771-8ea5-0cd27ac9db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"saved_models/f0/model0_best.h5\" #select fold\n",
    "params_file = 'params_pred.json' # from Calico repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab97ad2-29c0-4cee-babd-f17bb1e82c22",
   "metadata": {},
   "source": [
    "We transfer each checkpoint individually, by constructing the keras model from the checkpoint and then saving all weights to a dictionary.<br> We then translate the dictionary to a pytorch state_dict where keys match the current architecture. There are easier ways to do this (from the weights.h5 without having TF installed)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ca312c2-0372-42cb-bca8-9614317f2d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
      "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "#Take\n",
    "with open(params_file) as params_open :\n",
    "    params = json.load(params_open)\n",
    "    params_model = params['model']\n",
    "    params_train = params['train']\n",
    "    \n",
    "params_model['verbose'] = True\n",
    "\n",
    "seqnn_model = seqnn.SeqNN(params_model)\n",
    "seqnn_model.restore(model_file, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e7134b3-e2a1-46a8-89f4-1fea9dc9edc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get model weights from keras model\n",
    "\n",
    "layer_weight_dict = dict()\n",
    "for layer in seqnn_model.model.layers: \n",
    "    cfg = layer.get_config()\n",
    "    weights = layer.get_weights()\n",
    "    if len(weights) != 0:\n",
    "        layer_weight_dict[cfg['name']] = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f90374b-4828-4147-b6ce-74b19a6f37b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_weight_dict['conv_dna.conv_layer'] = layer_weight_dict['conv1d']\n",
    "for key, layer in layer_weight_dict.items():\n",
    "    temp_list = []\n",
    "    for weights in layer:\n",
    "        temp_list.append(torch.as_tensor(weights))\n",
    "    layer_weight_dict[key] = temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07af3629-977c-43dd-a7b6-565303441183",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transfer them to a pytorch state dict\n",
    "\n",
    "def transform_the_transformer(layers):\n",
    "    weights = dict()\n",
    "    last_layer = ''\n",
    "    transformer_layer=  0\n",
    "    prefix = f\"transformer.{transformer_layer}\"\n",
    "    for layer_name, layer in layers.items():\n",
    "        prefix = f\"transformer.{transformer_layer}\"\n",
    "        if 'sync_batch_normalization_6' in layer_name:\n",
    "            break\n",
    "        if  \"layer_normalization\" in layer_name and \"multihead\" not in last_layer :\n",
    "            weights[f\"{prefix}.0.fn.0.weight\"] = layer[0]\n",
    "            weights[f\"{prefix}.0.fn.0.bias\"] = layer[1]\n",
    "        if \"multihead\" in layer_name:\n",
    "            weights[f\"{prefix}.0.fn.1.rel_content_bias\"] = layer[0]\n",
    "            weights[f\"{prefix}.0.fn.1.rel_pos_bias\"] = layer[1]\n",
    "            weights[f\"{prefix}.0.fn.1.to_q.weight\"] = layer[2].T\n",
    "            weights[f\"{prefix}.0.fn.1.to_k.weight\"] = layer[3].T\n",
    "            weights[f\"{prefix}.0.fn.1.to_v.weight\"] = layer[4].T\n",
    "            weights[f\"{prefix}.0.fn.1.to_out.weight\"] = layer[5].T\n",
    "            weights[f\"{prefix}.0.fn.1.to_out.bias\"] = layer[6]\n",
    "            weights[f\"{prefix}.0.fn.1.to_rel_k.weight\"] = layer[7].T\n",
    "        if \"layer_normalization\" in layer_name and \"multihead\" in last_layer:\n",
    "            weights[f\"{prefix}.1.fn.0.weight\"] = layer[0]\n",
    "            weights[f\"{prefix}.1.fn.0.bias\"] = layer[1]\n",
    "        if \"dense\" in layer_name and  \"dense\" not in last_layer:\n",
    "            weights[f\"{prefix}.1.fn.1.weight\"] = layer[0].T\n",
    "            weights[f\"{prefix}.1.fn.1.bias\"] = layer[1]\n",
    "        if \"dense\" in layer_name and  \"dense\" in last_layer:\n",
    "            weights[f\"{prefix}.1.fn.4.weight\"] = layer[0].T\n",
    "            weights[f\"{prefix}.1.fn.4.bias\"] = layer[1]\n",
    "            transformer_layer += 1\n",
    "        last_layer = layer_name\n",
    "    return weights\n",
    "\n",
    "def convert_the_convs(layers):\n",
    "    weights = dict()\n",
    "    conv_lookup = {\"conv1d\" : \"conv_dna.conv_layer\",\n",
    "    \"conv1d_1\" : \"res_tower.0.conv_layer\",\n",
    "    \"conv1d_2\" : \"res_tower.2.conv_layer\",\n",
    "    \"conv1d_3\": \"res_tower.4.conv_layer\",\n",
    "    \"conv1d_4\": \"res_tower.6.conv_layer\",\n",
    "    \"conv1d_5\" : \"res_tower.8.conv_layer\",\n",
    "    \"conv1d_6\" : \"unet1.1.conv_layer\",\n",
    "    \"separable_conv1d\" : \"separable1.conv_layer\",\n",
    "    \"separable_conv1d_1\" : \"separable0.conv_layer\",\n",
    "    \"dense_16\": \"upsampling_unet1.0.conv_layer\",\n",
    "    \"dense_17\": \"horizontal_conv1.conv_layer\",\n",
    "    \"dense_18\" : \"upsampling_unet0.0.conv_layer\",\n",
    "    \"dense_19\" : \"horizontal_conv0.conv_layer\",\n",
    "    \"conv1d_7\" : \"final_joined_convs.0.conv_layer\",\n",
    "    \"dense_20\" : \"human_head\"\n",
    "                  }\n",
    "    for conv_tf, conv_pt in conv_lookup.items():\n",
    "        if 'separable' in conv_tf:\n",
    "            weights[f\"{conv_pt}.0.weight\"] = layers[conv_tf][0].permute((1,2,0))\n",
    "            weights[f\"{conv_pt}.1.weight\"] = layers[conv_tf][1].permute((2,1,0))\n",
    "            weights[f\"{conv_pt}.1.bias\"] = layers[conv_tf][2]\n",
    "        else:\n",
    "            try:\n",
    "                weights[f\"{conv_pt}.weight\"] = layers[conv_tf][0].permute((2,1,0))\n",
    "            except:\n",
    "                weights[f\"{conv_pt}.weight\"] = layers[conv_tf][0].unsqueeze(0).permute((2,1,0))\n",
    "            weights[f\"{conv_pt}.bias\"] = layers[conv_tf][1]\n",
    "    return weights\n",
    "\n",
    "def normalize_the_norms(layers):\n",
    "    weights = dict()\n",
    "    norm_lookup = {\"sync_batch_normalization\": \"res_tower.0.norm\",\n",
    "    \"sync_batch_normalization_1\":\"res_tower.2.norm\",\n",
    "    \"sync_batch_normalization_2\" : \"res_tower.4.norm\",\n",
    "    \"sync_batch_normalization_3\" : \"res_tower.6.norm\",\n",
    "    \"sync_batch_normalization_4\": \"res_tower.8.norm\",\n",
    "    \"sync_batch_normalization_5\": \"unet1.1.norm\",\n",
    "    \"sync_batch_normalization_6\": \"upsampling_unet1.0.norm\",\n",
    "    \"sync_batch_normalization_7\": \"horizontal_conv1.norm\",\n",
    "    \"sync_batch_normalization_8\": \"upsampling_unet0.0.norm\",\n",
    "    \"sync_batch_normalization_9\": \"horizontal_conv0.norm\",\n",
    "    \"sync_batch_normalization_10\": \"final_joined_convs.0.norm\",\n",
    "    }\n",
    "    for norm_tf, norm_pt in norm_lookup.items():\n",
    "        weights[f\"{norm_pt}.weight\"] = layers[norm_tf][0]\n",
    "        weights[f\"{norm_pt}.bias\"] = layers[norm_tf][1]\n",
    "        weights[f\"{norm_pt}.running_mean\"] = layers[norm_tf][2]\n",
    "        weights[f\"{norm_pt}.running_var\"] = layers[norm_tf][3]\n",
    "    return weights\n",
    "\n",
    "res_transformers = transform_the_transformer(layer_weight_dict)\n",
    "res_convs = convert_the_convs(layer_weight_dict)\n",
    "res_norms = normalize_the_norms(layer_weight_dict)\n",
    "\n",
    "\n",
    "\n",
    "z = {**res_transformers, **res_convs, **res_norms}           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5e575ec-a344-4791-8242-0b742d60a796",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(z, f\"{model_file}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3fae2ea-97db-461e-8862-429952ac41e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved_models/f0/model0_best.h5.pt'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{model_file}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bc4a75-83a4-4555-a9d0-907c4981bbbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda-borzoi]",
   "language": "python",
   "name": "conda-env-anaconda-borzoi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
